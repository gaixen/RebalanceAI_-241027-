{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dependencies"
      ],
      "metadata": {
        "id": "ydRzjm0wN4we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:29.962944Z",
          "iopub.execute_input": "2025-07-01T17:35:29.963844Z",
          "iopub.status.idle": "2025-07-01T17:35:29.968288Z",
          "shell.execute_reply.started": "2025-07-01T17:35:29.963816Z",
          "shell.execute_reply": "2025-07-01T17:35:29.967491Z"
        },
        "id": "LPuHNRfKFb1-"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:29.969595Z",
          "iopub.execute_input": "2025-07-01T17:35:29.969867Z",
          "iopub.status.idle": "2025-07-01T17:35:29.994407Z",
          "shell.execute_reply.started": "2025-07-01T17:35:29.969847Z",
          "shell.execute_reply": "2025-07-01T17:35:29.993710Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIFb9ReYFb2A",
        "outputId": "a06fea21-474d-41b0-fec1-4be937e231fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Device available: CUDA\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching the data"
      ],
      "metadata": {
        "id": "Zb0q4lRsN-E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Data fetching function\n",
        "def fetch_data(symbols, start_date, end_date):\n",
        "    \"\"\"Fetch data for multiple symbols\"\"\"\n",
        "    all_data = []\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            df = yf.download(symbol, start=start_date, end=end_date)\n",
        "            df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
        "            df['Symbol'] = symbol\n",
        "            all_data.append(df)\n",
        "            print(f\"✓ Fetched {len(df)} records for {symbol}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error fetching {symbol}: {e}\")\n",
        "\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"\\n📊 Total combined records: {len(combined_df)}\")\n",
        "        return combined_df.reset_index(drop=True) # Reset index here\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# %%\n",
        "# Technical indicators function\n",
        "def add_indicators(df):\n",
        "    \"\"\"Add comprehensive technical indicators to stock DataFrame.\"\"\"\n",
        "    import numpy as np\n",
        "    print(\"Adding technical indicators...\")\n",
        "    result_dfs = []\n",
        "\n",
        "    for symbol in df['Symbol'].unique():\n",
        "        symbol_df = df[df['Symbol'] == symbol].copy()\n",
        "        print(f\"Processing {symbol}...\")\n",
        "\n",
        "        # Moving Averages\n",
        "        symbol_df['SMA_10'] = symbol_df['Close'].rolling(window=10).mean()\n",
        "        symbol_df['SMA_20'] = symbol_df['Close'].rolling(window=20).mean()\n",
        "        symbol_df['EMA_12'] = symbol_df['Close'].ewm(span=12, adjust=False).mean()\n",
        "        symbol_df['EMA_26'] = symbol_df['Close'].ewm(span=26, adjust=False).mean()\n",
        "\n",
        "        # RSI\n",
        "        delta = symbol_df['Close'].diff()\n",
        "        gain = delta.clip(lower=0).rolling(window=14).mean()\n",
        "        loss = -delta.clip(upper=0).rolling(window=14).mean()\n",
        "        rs = gain / (loss + 1e-10)  # avoid divide by zero\n",
        "        symbol_df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # MACD\n",
        "        symbol_df['MACD'] = symbol_df['EMA_12'] - symbol_df['EMA_26']\n",
        "        symbol_df['MACD_Signal'] = symbol_df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "        symbol_df['MACD_Hist'] = symbol_df['MACD'] - symbol_df['MACD_Signal']\n",
        "\n",
        "        # Bollinger Bands\n",
        "        sma20 = symbol_df['Close'].rolling(window=20).mean()\n",
        "        std20 = symbol_df['Close'].rolling(window=20).std()\n",
        "        symbol_df['BB_Upper'] = sma20 + (2 * std20)\n",
        "        symbol_df['BB_Lower'] = sma20 - (2 * std20)\n",
        "        symbol_df['BB_Width'] = (symbol_df['BB_Upper'] - symbol_df['BB_Lower']) / sma20\n",
        "        symbol_df['BB_Position'] = (symbol_df['Close'] - symbol_df['BB_Lower']) / (symbol_df['BB_Upper'] - symbol_df['BB_Lower'] + 1e-10)\n",
        "\n",
        "        # ATR (Average True Range)\n",
        "        high_low = symbol_df['High'] - symbol_df['Low']\n",
        "        high_close = (symbol_df['High'] - symbol_df['Close'].shift()).abs()\n",
        "        low_close = (symbol_df['Low'] - symbol_df['Close'].shift()).abs()\n",
        "        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "        symbol_df['ATR'] = true_range.rolling(window=14).mean()\n",
        "\n",
        "        # Stochastic Oscillator\n",
        "        low14 = symbol_df['Low'].rolling(window=14).min()\n",
        "        high14 = symbol_df['High'].rolling(window=14).max()\n",
        "        denom = (high14 - low14).replace(0, 1e-10)\n",
        "        symbol_df['Stoch_K'] = 100 * ((symbol_df['Close'] - low14) / denom)\n",
        "        symbol_df['Stoch_D'] = symbol_df['Stoch_K'].rolling(window=3).mean()\n",
        "\n",
        "        # Momentum\n",
        "        symbol_df['Momentum'] = symbol_df['Close'] / symbol_df['Close'].shift(10) - 1\n",
        "        symbol_df['ROC'] = symbol_df['Close'].pct_change(10) * 100\n",
        "\n",
        "        # Volume-based\n",
        "        symbol_df['Volume_SMA'] = symbol_df['Volume'].rolling(window=20).mean()\n",
        "        symbol_df['Volume_Ratio'] = symbol_df['Volume'] / (symbol_df['Volume_SMA'] + 1e-10)\n",
        "\n",
        "        # Price position in recent range\n",
        "        low_20 = symbol_df['Low'].rolling(window=20).min()\n",
        "        high_20 = symbol_df['High'].rolling(window=20).max()\n",
        "        denom_pos = (high_20 - low_20).replace(0, 1e-10)\n",
        "        symbol_df['Price_Position'] = (symbol_df['Close'] - low_20) / denom_pos\n",
        "\n",
        "        # Volatility\n",
        "        symbol_df['Volatility'] = symbol_df['Close'].pct_change().rolling(20).std()\n",
        "\n",
        "        # Drop NaNs and add symbol's enriched data\n",
        "        symbol_df = symbol_df.dropna()\n",
        "        result_dfs.append(symbol_df)\n",
        "\n",
        "    final_df = pd.concat(result_dfs, ignore_index=True)\n",
        "    print(f\"✓ Technical indicators added. Final dataset: {len(final_df)} records\")\n",
        "    return final_df\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:29.995702Z",
          "iopub.execute_input": "2025-07-01T17:35:29.996019Z",
          "iopub.status.idle": "2025-07-01T17:35:30.015979Z",
          "shell.execute_reply.started": "2025-07-01T17:35:29.995993Z",
          "shell.execute_reply": "2025-07-01T17:35:30.015403Z"
        },
        "id": "Qq9kHLuXFb2B"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment and agents\n"
      ],
      "metadata": {
        "id": "INeuVU6tHn2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Enhanced Trading Environment\n",
        "class AdvancedTradingEnv(gym.Env):\n",
        "    \"\"\"Enhanced Gym environment for multi-stock trading\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, initial_balance=10000, max_steps=200, normalize=True):\n",
        "        super(AdvancedTradingEnv, self).__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.initial_balance = initial_balance\n",
        "        self.max_steps = min(max_steps, len(df) - 1)\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Shuffle the data to mix different stocks\n",
        "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Feature columns (excluding non-feature columns)\n",
        "        self.feature_columns = [\n",
        "            'Close', 'Volume', 'SMA_10', 'SMA_20', 'EMA_12', 'EMA_26',\n",
        "            'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'BB_Upper', 'BB_Lower',\n",
        "            'BB_Width', 'BB_Position', 'ATR', 'Stoch_K', 'Stoch_D',\n",
        "            'Momentum', 'ROC', 'Volume_Ratio', 'Price_Position', 'Volatility'\n",
        "        ]\n",
        "\n",
        "        # Normalize features if requested\n",
        "        if self.normalize:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.df[self.feature_columns] = self.scaler.fit_transform(self.df[self.feature_columns])\n",
        "\n",
        "        # Actions: 0 = hold, 1 = buy, 2 = sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: features + portfolio state\n",
        "        n_features = len(self.feature_columns) + 3  # +3 for shares_held, balance, net_worth\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(n_features,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = float(self.initial_balance)\n",
        "        self.net_worth = float(self.initial_balance)\n",
        "        self.shares_held = 0\n",
        "        self.current_step = 0\n",
        "        self.previous_net_worth = self.initial_balance\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        if self.current_step >= len(self.df):\n",
        "            self.current_step = len(self.df) - 1\n",
        "\n",
        "        row = self.df.iloc[self.current_step]\n",
        "\n",
        "        # Get feature values\n",
        "        features = [row[col] for col in self.feature_columns]\n",
        "\n",
        "        # Add portfolio state (normalized)\n",
        "        portfolio_features = [\n",
        "            self.shares_held / 100,  # Normalize shares\n",
        "            (self.balance - self.initial_balance) / self.initial_balance,  # Normalize balance\n",
        "            (self.net_worth - self.initial_balance) / self.initial_balance  # Normalize net worth\n",
        "        ]\n",
        "\n",
        "        obs = np.array(features + portfolio_features, dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= len(self.df) - 1:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        row = self.df.iloc[self.current_step]\n",
        "        price = row['Close']\n",
        "\n",
        "        # Denormalize price if needed\n",
        "        if self.normalize:\n",
        "            close_idx = self.feature_columns.index('Close')\n",
        "            price_array = np.zeros((1, len(self.feature_columns)))\n",
        "            price_array[0, close_idx] = price\n",
        "            original_price = self.scaler.inverse_transform(price_array)[0, close_idx]\n",
        "        else:\n",
        "            original_price = price\n",
        "\n",
        "        prev_net_worth = self.net_worth\n",
        "\n",
        "        # Execute action\n",
        "        if action == 1 and self.balance >= original_price:  # Buy\n",
        "            shares_to_buy = int(self.balance // original_price)\n",
        "            if shares_to_buy > 0:\n",
        "                self.shares_held += shares_to_buy\n",
        "                self.balance -= shares_to_buy * original_price\n",
        "        elif action == 2 and self.shares_held > 0:  # Sell\n",
        "            self.balance += self.shares_held * original_price\n",
        "            self.shares_held = 0\n",
        "\n",
        "        # Move to next step\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps or self.current_step >= len(self.df) - 1\n",
        "\n",
        "        # Calculate new net worth\n",
        "        if not done:\n",
        "            next_row = self.df.iloc[self.current_step]\n",
        "            next_price = next_row['Close']\n",
        "            if self.normalize:\n",
        "                next_price_array = np.zeros((1, len(self.feature_columns)))\n",
        "                next_price_array[0, close_idx] = next_price\n",
        "                next_original_price = self.scaler.inverse_transform(next_price_array)[0, close_idx]\n",
        "            else:\n",
        "                next_original_price = next_price\n",
        "            self.net_worth = self.balance + self.shares_held * next_original_price\n",
        "        else:\n",
        "            self.net_worth = self.balance + self.shares_held * original_price\n",
        "\n",
        "        # Calculate reward\n",
        "        raw_reward = self.net_worth - prev_net_worth\n",
        "        reward = raw_reward / self.initial_balance * 100  # Percentage-based reward\n",
        "\n",
        "        # Penalty for excessive trading\n",
        "        if action != 0:\n",
        "            reward -= 0.1  # Small transaction cost\n",
        "\n",
        "        self.previous_net_worth = self.net_worth\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        return obs, reward, done, {'net_worth': self.net_worth, 'balance': self.balance, 'shares': self.shares_held}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:30.112671Z",
          "iopub.execute_input": "2025-07-01T17:35:30.112952Z",
          "iopub.status.idle": "2025-07-01T17:35:30.126963Z",
          "shell.execute_reply.started": "2025-07-01T17:35:30.112933Z",
          "shell.execute_reply": "2025-07-01T17:35:30.126398Z"
        },
        "id": "z5LMufEmFb2C"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedDQN(nn.Module):\n",
        "    \"\"\"Enhanced DQN with Dueling architecture\"\"\"\n",
        "    def __init__(self, input_dim, action_dim, hidden_dim=512):\n",
        "        super(ImprovedDQN, self).__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "        self.value_head = nn.Linear(hidden_dim // 2, 1)\n",
        "        self.advantage_head = nn.Linear(hidden_dim // 2, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        # Dueling DQN architecture\n",
        "        value = self.value_head(features)\n",
        "        advantage = self.advantage_head(features)\n",
        "\n",
        "        # Combine value and advantage\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values\n",
        "\n",
        "print(\"✓ Enhanced DQN architecture defined\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:30.127868Z",
          "iopub.execute_input": "2025-07-01T17:35:30.128137Z",
          "iopub.status.idle": "2025-07-01T17:35:30.154783Z",
          "shell.execute_reply.started": "2025-07-01T17:35:30.128121Z",
          "shell.execute_reply": "2025-07-01T17:35:30.153963Z"
        },
        "id": "rrtUm4NkFb2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112d7b9c-0128-444e-f3e7-6135fa14a390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Enhanced DQN architecture defined\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Prioritized Experience Replay Buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity=50000, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.pos = 0\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
        "\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "        else:\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
        "\n",
        "        self.priorities[self.pos] = max_prio\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            prios = self.priorities\n",
        "        else:\n",
        "            prios = self.priorities[:self.pos]\n",
        "\n",
        "        probs = prios ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return samples, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, prio in zip(indices, priorities):\n",
        "            self.priorities[idx] = prio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "print(\"✓ Prioritized Experience Replay implemented\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:30.155978Z",
          "iopub.execute_input": "2025-07-01T17:35:30.156282Z",
          "iopub.status.idle": "2025-07-01T17:35:30.183662Z",
          "shell.execute_reply.started": "2025-07-01T17:35:30.156263Z",
          "shell.execute_reply": "2025-07-01T17:35:30.182923Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDwSnATdFb2D",
        "outputId": "b80a429e-383f-4bf1-da68-4cd39e896883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Prioritized Experience Replay implemented\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "eEm_cKBTHs2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching the data"
      ],
      "metadata": {
        "id": "f72TVNxNIXxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# symbols = ['META']\n",
        "symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'NVDA', 'META' , 'NFLX']\n",
        "print(f\"Training symbols: {symbols}\")\n",
        "\n",
        "df = fetch_data(symbols, '2018-01-01', '2023-01-01')\n",
        "\n",
        "if df.empty:\n",
        "    print(\"No data fetched.\")\n",
        "else:\n",
        "    print(f\"✅ Successfully fetched data for {len(df['Symbol'].unique())} symbols\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:39:35.897888Z",
          "iopub.execute_input": "2025-07-01T17:39:35.898294Z",
          "iopub.status.idle": "2025-07-01T17:39:36.662966Z",
          "shell.execute_reply.started": "2025-07-01T17:39:35.898268Z",
          "shell.execute_reply": "2025-07-01T17:39:36.661997Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mo9HYb9Fb2E",
        "outputId": "2647fb1a-d85d-4ccb-fbfb-e9cc9a5c1e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training symbols: ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'NVDA', 'META', 'NFLX']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for AAPL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for GOOGL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for MSFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for TSLA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for AMZN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for NVDA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fetched 1259 records for META\n",
            "✓ Fetched 1259 records for NFLX\n",
            "\n",
            "📊 Total combined records: 10072\n",
            "✅ Successfully fetched data for 8 symbols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9zfd6XbIoNG",
        "outputId": "2bc98062-0a3b-400b-adfe-0c3dac18ff61"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10072 entries, 0 to 10071\n",
            "Data columns (total 41 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   (Open, AAPL)     1259 non-null   float64\n",
            " 1   (High, AAPL)     1259 non-null   float64\n",
            " 2   (Low, AAPL)      1259 non-null   float64\n",
            " 3   (Close, AAPL)    1259 non-null   float64\n",
            " 4   (Volume, AAPL)   1259 non-null   float64\n",
            " 5   (Symbol, )       10072 non-null  object \n",
            " 6   (Open, GOOGL)    1259 non-null   float64\n",
            " 7   (High, GOOGL)    1259 non-null   float64\n",
            " 8   (Low, GOOGL)     1259 non-null   float64\n",
            " 9   (Close, GOOGL)   1259 non-null   float64\n",
            " 10  (Volume, GOOGL)  1259 non-null   float64\n",
            " 11  (Open, MSFT)     1259 non-null   float64\n",
            " 12  (High, MSFT)     1259 non-null   float64\n",
            " 13  (Low, MSFT)      1259 non-null   float64\n",
            " 14  (Close, MSFT)    1259 non-null   float64\n",
            " 15  (Volume, MSFT)   1259 non-null   float64\n",
            " 16  (Open, TSLA)     1259 non-null   float64\n",
            " 17  (High, TSLA)     1259 non-null   float64\n",
            " 18  (Low, TSLA)      1259 non-null   float64\n",
            " 19  (Close, TSLA)    1259 non-null   float64\n",
            " 20  (Volume, TSLA)   1259 non-null   float64\n",
            " 21  (Open, AMZN)     1259 non-null   float64\n",
            " 22  (High, AMZN)     1259 non-null   float64\n",
            " 23  (Low, AMZN)      1259 non-null   float64\n",
            " 24  (Close, AMZN)    1259 non-null   float64\n",
            " 25  (Volume, AMZN)   1259 non-null   float64\n",
            " 26  (Open, NVDA)     1259 non-null   float64\n",
            " 27  (High, NVDA)     1259 non-null   float64\n",
            " 28  (Low, NVDA)      1259 non-null   float64\n",
            " 29  (Close, NVDA)    1259 non-null   float64\n",
            " 30  (Volume, NVDA)   1259 non-null   float64\n",
            " 31  (Open, META)     1259 non-null   float64\n",
            " 32  (High, META)     1259 non-null   float64\n",
            " 33  (Low, META)      1259 non-null   float64\n",
            " 34  (Close, META)    1259 non-null   float64\n",
            " 35  (Volume, META)   1259 non-null   float64\n",
            " 36  (Open, NFLX)     1259 non-null   float64\n",
            " 37  (High, NFLX)     1259 non-null   float64\n",
            " 38  (Low, NFLX)      1259 non-null   float64\n",
            " 39  (Close, NFLX)    1259 non-null   float64\n",
            " 40  (Volume, NFLX)   1259 non-null   float64\n",
            "dtypes: float64(40), object(1)\n",
            "memory usage: 3.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the indicators"
      ],
      "metadata": {
        "id": "Z3-sAgEDIawF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def melt_stock_data(df):\n",
        "    \"\"\"Convert wide-format stock DataFrame to long-format.\"\"\"\n",
        "    df.columns = pd.MultiIndex.from_tuples([eval(col) if isinstance(col, str) else col for col in df.columns])\n",
        "\n",
        "    symbols = sorted(set(sym for (_, sym) in df.columns if sym not in (None, '')))\n",
        "    long_df = []\n",
        "\n",
        "    for sym in symbols:\n",
        "        sym_df = df.xs(key=sym, axis=1, level=1).copy()\n",
        "        sym_df['Symbol'] = sym\n",
        "        sym_df['Date'] = df.index\n",
        "        long_df.append(sym_df)\n",
        "\n",
        "    final_df = pd.concat(long_df, ignore_index=True)\n",
        "    final_df = final_df[['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "RYyjwPuqLZjn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_melted = melt_stock_data(df)\n",
        "df_with_indicators = add_indicators(df_melted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI_JhdrqHiVG",
        "outputId": "980bc5bd-299b-42d8-8f78-c7fc1f282136"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding technical indicators...\n",
            "Processing AAPL...\n",
            "Processing AMZN...\n",
            "Processing GOOGL...\n",
            "Processing META...\n",
            "Processing MSFT...\n",
            "Processing NFLX...\n",
            "Processing NVDA...\n",
            "Processing TSLA...\n",
            "✓ Technical indicators added. Final dataset: 9912 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7124b1b8",
        "outputId": "a66c2d6b-b340-42a1-b568-22c88605d4d4"
      },
      "source": [
        "print(f\"📈 Dataset ready with {len(df_with_indicators.columns)} features\")\n",
        "print(f\"Feature columns: {df_with_indicators.columns.tolist()}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n📊 Sample of processed data:\")\n",
        "print(df_with_indicators.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Dataset ready with 28 features\n",
            "Feature columns: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'SMA_20', 'EMA_12', 'EMA_26', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position', 'ATR', 'Stoch_K', 'Stoch_D', 'Momentum', 'ROC', 'Volume_SMA', 'Volume_Ratio', 'Price_Position', 'Volatility']\n",
            "\n",
            "📊 Sample of processed data:\n",
            "   Date Symbol       Open       High        Low      Close       Volume  \\\n",
            "0    20   AAPL  39.161870  39.530327  39.075038  39.293293  129915600.0   \n",
            "1    21   AAPL  39.232285  39.572577  39.136063  39.375443  188923200.0   \n",
            "2    22   AAPL  38.957690  39.145439  37.573051  37.666924  346375200.0   \n",
            "3    23   AAPL  37.338373  38.460167  36.610848  36.725845  290954000.0   \n",
            "4    24   AAPL  36.336262  38.422610  36.141473  38.260677  272975200.0   \n",
            "\n",
            "      SMA_10     SMA_20     EMA_12  ...  BB_Position       ATR    Stoch_K  \\\n",
            "0  40.623022  40.856886  40.334647  ...     0.046832  0.658962  17.727183   \n",
            "1  40.353604  40.804669  40.187077  ...     0.111679  0.670025  20.000177   \n",
            "2  39.932109  40.657638  39.799361  ...    -0.145890  0.763900   1.999965   \n",
            "3  39.450771  40.440438  39.326513  ...    -0.141584  0.841514   2.033216   \n",
            "4  39.121976  40.307606  39.162538  ...     0.163639  0.934382  34.597632   \n",
            "\n",
            "     Stoch_D  Momentum        ROC   Volume_SMA  Volume_Ratio  Price_Position  \\\n",
            "0  13.099271 -0.065159  -6.515948  126824700.0      1.024371        0.177272   \n",
            "1  17.489206 -0.064041  -6.404097  130367280.0      1.449161        0.200002   \n",
            "2  13.242442 -0.100639 -10.063885  143199120.0      2.418836        0.020000   \n",
            "3   8.011119 -0.115876 -11.587564  153014820.0      1.901476        0.020332   \n",
            "4  12.876938 -0.079135  -7.913487  162550020.0      1.679330        0.345976   \n",
            "\n",
            "   Volatility  \n",
            "0    0.009462  \n",
            "1    0.009490  \n",
            "2    0.013250  \n",
            "3    0.013567  \n",
            "4    0.017207  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "B8jGycilIi_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_multi_stock_dqn(df, episodes=1000, max_steps=300):\n",
        "    \"\"\"Train the multi-stock DQN\"\"\"\n",
        "\n",
        "    # Hyperparameters\n",
        "    gamma = 0.99\n",
        "    epsilon_start = 1.0\n",
        "    epsilon_end = 0.05\n",
        "    epsilon_decay = 0.995\n",
        "    batch_size = 128\n",
        "    target_update = 20\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    print(f\"🎯 Training Configuration:\")\n",
        "    print(f\"   Episodes: {episodes}\")\n",
        "    print(f\"   Max steps per episode: {max_steps}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Learning rate: {learning_rate}\")\n",
        "\n",
        "    # Initialize environment\n",
        "    env = AdvancedTradingEnv(df, initial_balance=10000, max_steps=max_steps, normalize=True)\n",
        "    print(f\"   Observation space: {env.observation_space.shape}\")\n",
        "    print(f\"   Action space: {env.action_space.n}\")\n",
        "\n",
        "    # Initialize networks\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    policy_net = ImprovedDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    target_net = ImprovedDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)\n",
        "    memory = PrioritizedReplayBuffer(capacity=50000)\n",
        "\n",
        "    # Training loop\n",
        "    epsilon = epsilon_start\n",
        "    episode_rewards = []\n",
        "    episode_net_worths = []\n",
        "    best_reward = float('-inf')\n",
        "\n",
        "    print(\"\\n🏋️ Starting training...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randrange(env.action_space.n)\n",
        "            else:\n",
        "                policy_net.eval() # Set policy_net to evaluation mode\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = q_values.argmax().item()\n",
        "                policy_net.train() # Set policy_net back to training mode\n",
        "\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Training\n",
        "            if len(memory) >= batch_size:\n",
        "                samples, indices, weights = memory.sample(batch_size)\n",
        "\n",
        "                states = torch.FloatTensor([s[0] for s in samples]).to(device)\n",
        "                actions = torch.LongTensor([s[1] for s in samples]).to(device)\n",
        "                rewards = torch.FloatTensor([s[2] for s in samples]).to(device)\n",
        "                next_states = torch.FloatTensor([s[3] for s in samples]).to(device)\n",
        "                dones = torch.BoolTensor([s[4] for s in samples]).to(device)\n",
        "                weights_tensor = torch.FloatTensor(weights).to(device)\n",
        "\n",
        "                current_q_values = policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "                next_q_values = target_net(next_states).max(1)[0].detach()\n",
        "                target_q_values = rewards + (gamma * next_q_values * ~dones)\n",
        "\n",
        "                # Compute loss with importance sampling weights\n",
        "                td_errors = current_q_values.squeeze() - target_q_values\n",
        "                loss = (weights_tensor * td_errors.pow(2)).mean()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update priorities\n",
        "                priorities = np.abs(td_errors.detach().cpu().numpy()) + 1e-6\n",
        "                memory.update_priorities(indices, priorities)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update target network\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        # Decay epsilon and update scheduler\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track performance\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_net_worths.append(info.get('net_worth', env.initial_balance))\n",
        "\n",
        "        # Save best model\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(policy_net.state_dict(), 'best_dqn_model.pth')\n",
        "\n",
        "        # Progress reporting\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            avg_net_worth = np.mean(episode_net_worths[-50:])\n",
        "            print(f\"Episode {episode:4d}/{episodes} | \"\n",
        "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
        "                  f\"Avg Net Worth: ${avg_net_worth:8.2f} | \"\n",
        "                  f\"Epsilon: {epsilon:.3f} | \"\n",
        "                  f\"Buffer: {len(memory):5d}\")\n",
        "\n",
        "    print(\"✅ Training completed!\")\n",
        "    return policy_net, env, episode_rewards, episode_net_worths"
      ],
      "metadata": {
        "id": "yhrMdQULL4lA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_env, training_rewards, training_net_worths = train_multi_stock_dqn(\n",
        "    df_with_indicators,\n",
        "    episodes=800,\n",
        "    max_steps=250\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIUDLkGyMGku",
        "outputId": "21c49381-4928-4a41-f88a-be4f47829d60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Training Configuration:\n",
            "   Episodes: 800\n",
            "   Max steps per episode: 250\n",
            "   Batch size: 128\n",
            "   Learning rate: 0.0001\n",
            "   Observation space: (25,)\n",
            "   Action space: 3\n",
            "   Device: cuda\n",
            "\n",
            "🏋️ Starting training...\n",
            "Episode    0/800 | Avg Reward:  -113.22 | Avg Net Worth: $  387.85 | Epsilon: 0.995 | Buffer:   250\n",
            "Episode   10/800 | Avg Reward: 129143366049.93 | Avg Net Worth: $12914336616719.36 | Epsilon: 0.946 | Buffer:  2750\n",
            "Episode   20/800 | Avg Reward: 67655250509.34 | Avg Net Worth: $6765525062633.33 | Epsilon: 0.900 | Buffer:  5250\n",
            "Episode   30/800 | Avg Reward: 45946443123.71 | Avg Net Worth: $4594644324071.96 | Epsilon: 0.856 | Buffer:  7750\n",
            "Episode   40/800 | Avg Reward: 34740344681.88 | Avg Net Worth: $3474034479880.90 | Epsilon: 0.814 | Buffer: 10250\n",
            "Episode   50/800 | Avg Reward: 28495624418.44 | Avg Net Worth: $2849562453537.56 | Epsilon: 0.774 | Buffer: 12750\n",
            "Episode   60/800 | Avg Reward: 49400291566.17 | Avg Net Worth: $4940029168299.03 | Epsilon: 0.737 | Buffer: 15250\n",
            "Episode   70/800 | Avg Reward: 250673682604.94 | Avg Net Worth: $25067368272209.93 | Epsilon: 0.701 | Buffer: 17750\n",
            "Episode   80/800 | Avg Reward: 250602149353.68 | Avg Net Worth: $25060214947127.62 | Epsilon: 0.666 | Buffer: 20250\n",
            "Episode   90/800 | Avg Reward: 276243475076.27 | Avg Net Worth: $27624347519441.86 | Epsilon: 0.634 | Buffer: 22750\n",
            "Episode  100/800 | Avg Reward: 278170730698.48 | Avg Net Worth: $27817073081719.18 | Epsilon: 0.603 | Buffer: 25250\n",
            "Episode  110/800 | Avg Reward: 279950985956.12 | Avg Net Worth: $27995098607552.12 | Epsilon: 0.573 | Buffer: 27750\n",
            "Episode  120/800 | Avg Reward: 78674587639.73 | Avg Net Worth: $7867458775952.50 | Epsilon: 0.545 | Buffer: 30250\n",
            "Episode  130/800 | Avg Reward: 79567909093.77 | Avg Net Worth: $7956790921386.58 | Epsilon: 0.519 | Buffer: 32750\n",
            "Episode  140/800 | Avg Reward: 53926312369.60 | Avg Net Worth: $5392631248772.82 | Epsilon: 0.493 | Buffer: 35250\n",
            "Episode  150/800 | Avg Reward: 51990521386.32 | Avg Net Worth: $5199052150222.42 | Epsilon: 0.469 | Buffer: 37750\n",
            "Episode  160/800 | Avg Reward: 894062126.00 | Avg Net Worth: $89406223952.18 | Epsilon: 0.446 | Buffer: 40250\n",
            "Episode  170/800 | Avg Reward: 894185810.34 | Avg Net Worth: $89418592153.66 | Epsilon: 0.424 | Buffer: 42750\n",
            "Episode  180/800 | Avg Reward: 808236.33 | Avg Net Worth: $80834505.07 | Epsilon: 0.404 | Buffer: 45250\n",
            "Episode  190/800 | Avg Reward: 14579524.44 | Avg Net Worth: $1457963300.78 | Epsilon: 0.384 | Buffer: 47750\n",
            "Episode  200/800 | Avg Reward: 14579479.71 | Avg Net Worth: $1457958840.63 | Epsilon: 0.365 | Buffer: 50000\n",
            "Episode  210/800 | Avg Reward: 15453156.31 | Avg Net Worth: $1545326535.75 | Epsilon: 0.347 | Buffer: 50000\n",
            "Episode  220/800 | Avg Reward: 14762459.05 | Avg Net Worth: $1476256869.96 | Epsilon: 0.330 | Buffer: 50000\n",
            "Episode  230/800 | Avg Reward: 3213144231.94 | Avg Net Worth: $321314434224.86 | Epsilon: 0.314 | Buffer: 50000\n",
            "Episode  240/800 | Avg Reward: 3199418992.69 | Avg Net Worth: $319941910350.04 | Epsilon: 0.299 | Buffer: 50000\n",
            "Episode  250/800 | Avg Reward: 3199412991.75 | Avg Net Worth: $319941310347.30 | Epsilon: 0.284 | Buffer: 50000\n",
            "Episode  260/800 | Avg Reward: 3198536413.57 | Avg Net Worth: $319853652626.25 | Epsilon: 0.270 | Buffer: 50000\n",
            "Episode  270/800 | Avg Reward: 3198465105.48 | Avg Net Worth: $319846521967.01 | Epsilon: 0.257 | Buffer: 50000\n",
            "Episode  280/800 | Avg Reward: 113573.00 | Avg Net Worth: $11368941.02 | Epsilon: 0.245 | Buffer: 50000\n",
            "Episode  290/800 | Avg Reward: 51204.91 | Avg Net Worth: $5132370.06 | Epsilon: 0.233 | Buffer: 50000\n",
            "Episode  300/800 | Avg Reward: 94123.72 | Avg Net Worth: $9424435.58 | Epsilon: 0.221 | Buffer: 50000\n",
            "Episode  310/800 | Avg Reward: 536930.89 | Avg Net Worth: $53705287.58 | Epsilon: 0.210 | Buffer: 50000\n",
            "Episode  320/800 | Avg Reward: 549072.71 | Avg Net Worth: $54919510.25 | Epsilon: 0.200 | Buffer: 50000\n",
            "Episode  330/800 | Avg Reward: 519145.00 | Avg Net Worth: $51926751.50 | Epsilon: 0.190 | Buffer: 50000\n",
            "Episode  340/800 | Avg Reward: 777804.16 | Avg Net Worth: $77792674.12 | Epsilon: 0.181 | Buffer: 50000\n",
            "Episode  350/800 | Avg Reward: 2458353.50 | Avg Net Worth: $245847617.14 | Epsilon: 0.172 | Buffer: 50000\n",
            "Episode  360/800 | Avg Reward: 2295223.79 | Avg Net Worth: $229534689.47 | Epsilon: 0.164 | Buffer: 50000\n",
            "Episode  370/800 | Avg Reward: 2294492.31 | Avg Net Worth: $229461588.10 | Epsilon: 0.156 | Buffer: 50000\n",
            "Episode  380/800 | Avg Reward: 2591691.64 | Avg Net Worth: $259181525.64 | Epsilon: 0.148 | Buffer: 50000\n",
            "Episode  390/800 | Avg Reward: 2338229.66 | Avg Net Worth: $233835336.63 | Epsilon: 0.141 | Buffer: 50000\n",
            "Episode  400/800 | Avg Reward: 2330262.99 | Avg Net Worth: $233038675.29 | Epsilon: 0.134 | Buffer: 50000\n",
            "Episode  410/800 | Avg Reward: 3598363.99 | Avg Net Worth: $359848775.37 | Epsilon: 0.127 | Buffer: 50000\n",
            "Episode  420/800 | Avg Reward: 3568720.64 | Avg Net Worth: $356884445.66 | Epsilon: 0.121 | Buffer: 50000\n",
            "Episode  430/800 | Avg Reward: 3271093.49 | Avg Net Worth: $327121474.68 | Epsilon: 0.115 | Buffer: 50000\n",
            "Episode  440/800 | Avg Reward: 3265683.33 | Avg Net Worth: $326580014.73 | Epsilon: 0.110 | Buffer: 50000\n",
            "Episode  450/800 | Avg Reward: 1749652.75 | Avg Net Worth: $174976522.51 | Epsilon: 0.104 | Buffer: 50000\n",
            "Episode  460/800 | Avg Reward: 201737.96 | Avg Net Worth: $20184603.92 | Epsilon: 0.099 | Buffer: 50000\n",
            "Episode  470/800 | Avg Reward: 201078.72 | Avg Net Worth: $20118237.93 | Epsilon: 0.094 | Buffer: 50000\n",
            "Episode  480/800 | Avg Reward: 205236.99 | Avg Net Worth: $20533875.85 | Epsilon: 0.090 | Buffer: 50000\n",
            "Episode  490/800 | Avg Reward: 205331.37 | Avg Net Worth: $20543309.73 | Epsilon: 0.085 | Buffer: 50000\n",
            "Episode  500/800 | Avg Reward:  9284.57 | Avg Net Worth: $938617.49 | Epsilon: 0.081 | Buffer: 50000\n",
            "Episode  510/800 | Avg Reward:  8705.50 | Avg Net Worth: $880712.50 | Epsilon: 0.077 | Buffer: 50000\n",
            "Episode  520/800 | Avg Reward:  8798.88 | Avg Net Worth: $890051.02 | Epsilon: 0.073 | Buffer: 50000\n",
            "Episode  530/800 | Avg Reward:  4651.83 | Avg Net Worth: $475382.48 | Epsilon: 0.070 | Buffer: 50000\n",
            "Episode  540/800 | Avg Reward:  4878.66 | Avg Net Worth: $498107.52 | Epsilon: 0.066 | Buffer: 50000\n",
            "Episode  550/800 | Avg Reward:  1090.08 | Avg Net Worth: $119470.46 | Epsilon: 0.063 | Buffer: 50000\n",
            "Episode  560/800 | Avg Reward:  1068.93 | Avg Net Worth: $117725.50 | Epsilon: 0.060 | Buffer: 50000\n",
            "Episode  570/800 | Avg Reward:   941.88 | Avg Net Worth: $105279.73 | Epsilon: 0.057 | Buffer: 50000\n",
            "Episode  580/800 | Avg Reward:   903.81 | Avg Net Worth: $101687.56 | Epsilon: 0.054 | Buffer: 50000\n",
            "Episode  590/800 | Avg Reward:   240.22 | Avg Net Worth: $35585.63 | Epsilon: 0.052 | Buffer: 50000\n",
            "Episode  600/800 | Avg Reward:   751.68 | Avg Net Worth: $86751.36 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  610/800 | Avg Reward:   880.03 | Avg Net Worth: $99300.30 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  620/800 | Avg Reward:   869.27 | Avg Net Worth: $97969.13 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  630/800 | Avg Reward:  2152.41 | Avg Net Worth: $226069.11 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  640/800 | Avg Reward:  2196.01 | Avg Net Worth: $230202.42 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  650/800 | Avg Reward:  5159.62 | Avg Net Worth: $526405.29 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  660/800 | Avg Reward: 11652.56 | Avg Net Worth: $1175717.07 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  670/800 | Avg Reward: 11685.25 | Avg Net Worth: $1179136.56 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  680/800 | Avg Reward: 15932.23 | Avg Net Worth: $1603947.09 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  690/800 | Avg Reward: 16907.98 | Avg Net Worth: $1701702.43 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  700/800 | Avg Reward: 17544.75 | Avg Net Worth: $1765573.48 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  710/800 | Avg Reward: 2096549.11 | Avg Net Worth: $209666239.87 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  720/800 | Avg Reward: 2101479.79 | Avg Net Worth: $210159608.84 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  730/800 | Avg Reward: 12709048.33 | Avg Net Worth: $1270916741.02 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  740/800 | Avg Reward: 12735138.95 | Avg Net Worth: $1273526008.84 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  750/800 | Avg Reward: 12731029.11 | Avg Net Worth: $1273115209.17 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  760/800 | Avg Reward: 10645797.00 | Avg Net Worth: $1064592120.61 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  770/800 | Avg Reward: 10642186.73 | Avg Net Worth: $1064231094.22 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  780/800 | Avg Reward: 29077.59 | Avg Net Worth: $2920218.89 | Epsilon: 0.050 | Buffer: 50000\n",
            "Episode  790/800 | Avg Reward:  2114.45 | Avg Net Worth: $223905.13 | Epsilon: 0.050 | Buffer: 50000\n",
            "✅ Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "J9iYtr7GNxIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(policy_net, test_symbols, start_date, end_date):\n",
        "    \"\"\"Evaluate the model on test data\"\"\"\n",
        "    print(\"🧪 Evaluating model on test data...\")\n",
        "\n",
        "    # Fetch test data\n",
        "    test_df = fetch_data(test_symbols, start_date, end_date)\n",
        "    test_df = add_indicators(test_df)\n",
        "\n",
        "    # Create test environment\n",
        "    test_env = AdvancedTradingEnv(test_df, initial_balance=10000, max_steps=200, normalize=True)\n",
        "\n",
        "    # Test the trained model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    state = test_env.reset()\n",
        "    dqn_values = [test_env.initial_balance]\n",
        "    actions_taken = []\n",
        "\n",
        "    policy_net.eval()\n",
        "    with torch.no_grad():\n",
        "        for step in range(test_env.max_steps):\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            q_values = policy_net(state_tensor)\n",
        "            action = q_values.argmax().item()\n",
        "\n",
        "            state, _, done, _ = test_env.step(action)\n",
        "            dqn_values.append(test_env.net_worth)\n",
        "            actions_taken.append(action)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    # Calculate buy-and-hold baseline\n",
        "    test_prices = []\n",
        "    for i in range(len(dqn_values)):\n",
        "        if i < len(test_df):\n",
        "            price = test_df.iloc[i]['Close']\n",
        "            if test_env.normalize:\n",
        "                close_idx = test_env.feature_columns.index('Close')\n",
        "                price_array = np.zeros((1, len(test_env.feature_columns)))\n",
        "                price_array[0, close_idx] = price\n",
        "                original_price = test_env.scaler.inverse_transform(price_array)[0, close_idx]\n",
        "            else:\n",
        "                original_price = price\n",
        "            test_prices.append(original_price)\n",
        "\n",
        "    if test_prices:\n",
        "        buy_hold_values = [(price / test_prices[0]) * test_env.initial_balance for price in test_prices]\n",
        "    else:\n",
        "        buy_hold_values = [test_env.initial_balance] * len(dqn_values)\n",
        "\n",
        "    print(f\"✅ Evaluation completed on {len(dqn_values)} time steps\")\n",
        "    return dqn_values, buy_hold_values, actions_taken"
      ],
      "metadata": {
        "id": "opRft93VM2oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "test_symbols = ['AAPL', 'GOOGL', 'MSFT']  # Subset for testing\n",
        "dqn_values, buy_hold_values, actions = evaluate_model(\n",
        "    trained_model, test_symbols, '2023-01-01', '2024-01-01'\n",
        ")"
      ],
      "metadata": {
        "id": "3ky_hnK4M7k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_comprehensive_results(dqn_values, buy_hold_values, training_rewards, actions):\n",
        "    \"\"\"Plot comprehensive results with multiple metrics\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # 1. Portfolio Value Comparison\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    plt.plot(dqn_values, label='DQN Agent', linewidth=2.5, color='#2E8B57')\n",
        "    plt.plot(buy_hold_values, label='Buy & Hold', linewidth=2.5, color='#FF6347')\n",
        "    plt.title('Portfolio Value Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Portfolio Value ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Training Progress\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    plt.plot(training_rewards, alpha=0.6, color='lightblue')\n",
        "    smoothed = pd.Series(training_rewards).rolling(50).mean()\n",
        "    plt.plot(smoothed, color='darkblue', linewidth=2)\n",
        "    plt.title('Training Rewards Over Episodes', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Action Distribution\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    action_names = ['Hold', 'Buy', 'Sell']\n",
        "    action_counts = [actions.count(i) for i in range(3)]\n",
        "    colors = ['#FFD700', '#32CD32', '#FF4500']\n",
        "    plt.pie(action_counts, labels=action_names, autopct='%1.1f%%', colors=colors)\n",
        "    plt.title('Action Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 4. Performance Metrics Bar Chart\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    dqn_return = (dqn_values[-1] - dqn_values[0]) / dqn_values[0] * 100\n",
        "    bh_return = (buy_hold_values[-1] - buy_hold_values[0]) / buy_hold_values[0] * 100\n",
        "\n",
        "    metrics = ['Total Return (%)', 'Final Value ($)', 'Max Value ($)']\n",
        "    dqn_metrics = [dqn_return, dqn_values[-1], max(dqn_values)]\n",
        "    bh_metrics = [bh_return, buy_hold_values[-1], max(buy_hold_values)]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, dqn_metrics, width, label='DQN Agent', alpha=0.8, color='#2E8B57')\n",
        "    plt.bar(x + width/2, bh_metrics, width, label='Buy & Hold', alpha=0.8, color='#FF6347')\n",
        "    plt.title('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Return Distribution\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    dqn_returns = np.diff(dqn_values) / dqn_values[:-1]\n",
        "    bh_returns = np.diff(buy_hold_values) / buy_hold_values[:-1]\n",
        "\n",
        "    plt.hist(dqn_returns, bins=30, alpha=0.7, label='DQN Returns', density=True, color='#2E8B57')\n",
        "    plt.hist(bh_returns, bins=30, alpha=0.7, label='Buy & Hold Returns', density=True, color='#FF6347')\n",
        "    plt.title('Return Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Daily Returns')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Cumulative Returns\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    dqn_cum_returns = np.cumprod(1 + dqn_returns) - 1\n",
        "    bh_cum_returns = np.cumprod(1 + bh_returns) - 1\n",
        "\n",
        "    plt.plot(dqn_cum_returns * 100, label='DQN Cumulative Returns', linewidth=2, color='#2E8B57')\n",
        "    plt.plot(bh_cum_returns * 100, label='Buy & Hold Cumulative Returns', linewidth=2, color='#FF6347')\n",
        "    plt.title('Cumulative Returns (%)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Cumulative Return (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and display metrics\n",
        "    dqn_volatility = np.std(dqn_returns) * np.sqrt(252)  # Annualized\n",
        "    bh_volatility = np.std(bh_returns) * np.sqrt(252)\n",
        "\n",
        "    dqn_sharpe = (np.mean(dqn_returns) * 252) / dqn_volatility if dqn_volatility > 0 else 0\n",
        "    bh_sharpe = (np.mean(bh_returns) * 252) / bh_volatility if bh_volatility > 0 else 0\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    dqn_max_drawdown = calculate_max_drawdown(dqn_values)\n",
        "    bh_max_drawdown = calculate_max_drawdown(buy_hold_values)\n",
        "\n",
        "    # Calculate win rate for DQN\n",
        "    positive_returns = sum(1 for r in dqn_returns if r > 0)\n",
        "    win_rate = positive_returns / len(dqn_returns) * 100 if len(dqn_returns) > 0 else 0\n",
        "\n",
        "    # Print comprehensive metrics summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\n📊 PORTFOLIO PERFORMANCE:\")\n",
        "    print(f\"{'Metric':<25} {'DQN Agent':<15} {'Buy & Hold':<15} {'Difference':<15}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Total Return (%)':<25} {dqn_return:<15.2f} {bh_return:<15.2f} {dqn_return - bh_return:<15.2f}\")\n",
        "    print(f\"{'Final Value ($)':<25} {dqn_values[-1]:<15.2f} {buy_hold_values[-1]:<15.2f} {dqn_values[-1] - buy_hold_values[-1]:<15.2f}\")\n",
        "    print(f\"{'Maximum Value ($)':<25} {max(dqn_values):<15.2f} {max(buy_hold_values):<15.2f} {max(dqn_values) - max(buy_hold_values):<15.2f}\")\n",
        "\n",
        "    print(f\"\\n📈 RISK METRICS:\")\n",
        "    print(f\"{'Volatility (Annual %)':<25} {dqn_volatility*100:<15.2f} {bh_volatility*100:<15.2f} {(dqn_volatility-bh_volatility)*100:<15.2f}\")\n",
        "    print(f\"{'Sharpe Ratio':<25} {dqn_sharpe:<15.2f} {bh_sharpe:<15.2f} {dqn_sharpe - bh_sharpe:<15.2f}\")\n",
        "    print(f\"{'Max Drawdown (%)':<25} {dqn_max_drawdown:<15.2f} {bh_max_drawdown:<15.2f} {dqn_max_drawdown - bh_max_drawdown:<15.2f}\")\n",
        "\n",
        "    print(f\"\\n🎯 TRADING BEHAVIOR:\")\n",
        "    print(f\"Win Rate (DQN): {win_rate:.1f}%\")\n",
        "    print(f\"Total Actions: {len(actions)}\")\n",
        "    print(f\"  - Hold: {action_counts[0]} ({action_counts[0]/len(actions)*100:.1f}%)\")\n",
        "    print(f\"  - Buy:  {action_counts[1]} ({action_counts[1]/len(actions)*100:.1f}%)\")\n",
        "    print(f\"  - Sell: {action_counts[2]} ({action_counts[2]/len(actions)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n🏆 SUMMARY:\")\n",
        "    if dqn_return > bh_return:\n",
        "        outperformance = dqn_return - bh_return\n",
        "        print(f\"✅ DQN Agent OUTPERFORMED Buy & Hold by {outperformance:.2f}%\")\n",
        "    else:\n",
        "        underperformance = bh_return - dqn_return\n",
        "        print(f\"❌ DQN Agent UNDERPERFORMED Buy & Hold by {underperformance:.2f}%\")\n",
        "\n",
        "    if dqn_sharpe > bh_sharpe:\n",
        "        print(f\"✅ DQN Agent achieved better risk-adjusted returns (Sharpe: {dqn_sharpe:.2f} vs {bh_sharpe:.2f})\")\n",
        "    else:\n",
        "        print(f\"❌ Buy & Hold achieved better risk-adjusted returns (Sharpe: {bh_sharpe:.2f} vs {dqn_sharpe:.2f})\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return {\n",
        "        'dqn_metrics': {\n",
        "            'total_return': dqn_return,\n",
        "            'final_value': dqn_values[-1],\n",
        "            'max_value': max(dqn_values),\n",
        "            'volatility': dqn_volatility,\n",
        "            'sharpe_ratio': dqn_sharpe,\n",
        "            'max_drawdown': dqn_max_drawdown,\n",
        "            'win_rate': win_rate\n",
        "        },\n",
        "        'bh_metrics': {\n",
        "            'total_return': bh_return,\n",
        "            'final_value': buy_hold_values[-1],\n",
        "            'max_value': max(buy_hold_values),\n",
        "            'volatility': bh_volatility,\n",
        "            'sharpe_ratio': bh_sharpe,\n",
        "            'max_drawdown': bh_max_drawdown\n",
        "        },\n",
        "        'action_distribution': dict(zip(action_names, action_counts))\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_max_drawdown(values):\n",
        "    \"\"\"Calculate maximum drawdown percentage\"\"\"\n",
        "    peak = values[0]\n",
        "    max_dd = 0\n",
        "\n",
        "    for value in values:\n",
        "        if value > peak:\n",
        "            peak = value\n",
        "        drawdown = (peak - value) / peak * 100\n",
        "        if drawdown > max_dd:\n",
        "            max_dd = drawdown\n",
        "\n",
        "    return max_dd"
      ],
      "metadata": {
        "trusted": true,
        "id": "-zXzkiT5Fb2F"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}