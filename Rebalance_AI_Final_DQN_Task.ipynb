{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaixen/RebalanceAI_-241027-/blob/main/Rebalance_AI_Final_DQN_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dependencies"
      ],
      "metadata": {
        "id": "ydRzjm0wN4we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:29.962944Z",
          "iopub.execute_input": "2025-07-01T17:35:29.963844Z",
          "iopub.status.idle": "2025-07-01T17:35:29.968288Z",
          "shell.execute_reply.started": "2025-07-01T17:35:29.963816Z",
          "shell.execute_reply": "2025-07-01T17:35:29.967491Z"
        },
        "id": "LPuHNRfKFb1-"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:29.969595Z",
          "iopub.execute_input": "2025-07-01T17:35:29.969867Z",
          "iopub.status.idle": "2025-07-01T17:35:29.994407Z",
          "shell.execute_reply.started": "2025-07-01T17:35:29.969847Z",
          "shell.execute_reply": "2025-07-01T17:35:29.993710Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HIFb9ReYFb2A",
        "outputId": "699cf54a-2f15-49aa-d048-ac8063931127"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "execution_count": 43
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching the data"
      ],
      "metadata": {
        "id": "Zb0q4lRsN-E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data(symbols, start_date, end_date):\n",
        "    \"\"\"Fetch data for multiple symbols\"\"\"\n",
        "    all_data = []\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            df = yf.download(symbol, start=start_date, end=end_date)\n",
        "            df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
        "            df['Symbol'] = symbol\n",
        "            all_data.append(df)\n",
        "            print(f\"âœ“ Fetched {len(df)} records for {symbol}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching {symbol}: {e}\")\n",
        "\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"\\nðŸ“Š Total combined records: {len(combined_df)}\")\n",
        "        return combined_df.reset_index(drop=True) # Reset index here\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def add_indicators(df):\n",
        "    \"\"\"Add comprehensive technical indicators to stock DataFrame.\"\"\"\n",
        "    import numpy as np\n",
        "    print(\"Adding technical indicators...\")\n",
        "    result_dfs = []\n",
        "\n",
        "    for symbol in df['Symbol'].unique():\n",
        "        symbol_df = df[df['Symbol'] == symbol].copy()\n",
        "        print(f\"Processing {symbol}...\")\n",
        "\n",
        "        # Moving Averages\n",
        "        symbol_df['SMA_10'] = symbol_df['Close'].rolling(window=10).mean()\n",
        "        symbol_df['SMA_20'] = symbol_df['Close'].rolling(window=20).mean()\n",
        "        symbol_df['EMA_12'] = symbol_df['Close'].ewm(span=12, adjust=False).mean()\n",
        "        symbol_df['EMA_26'] = symbol_df['Close'].ewm(span=26, adjust=False).mean()\n",
        "\n",
        "        # RSI\n",
        "        delta = symbol_df['Close'].diff()\n",
        "        gain = delta.clip(lower=0).rolling(window=14).mean()\n",
        "        loss = -delta.clip(upper=0).rolling(window=14).mean()\n",
        "        rs = gain / (loss + 1e-10)  # avoid divide by zero\n",
        "        symbol_df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # MACD\n",
        "        symbol_df['MACD'] = symbol_df['EMA_12'] - symbol_df['EMA_26']\n",
        "        symbol_df['MACD_Signal'] = symbol_df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "        symbol_df['MACD_Hist'] = symbol_df['MACD'] - symbol_df['MACD_Signal']\n",
        "\n",
        "        # Bollinger Bands\n",
        "        sma20 = symbol_df['Close'].rolling(window=20).mean()\n",
        "        std20 = symbol_df['Close'].rolling(window=20).std()\n",
        "        symbol_df['BB_Upper'] = sma20 + (2 * std20)\n",
        "        symbol_df['BB_Lower'] = sma20 - (2 * std20)\n",
        "        symbol_df['BB_Width'] = (symbol_df['BB_Upper'] - symbol_df['BB_Lower']) / sma20\n",
        "        symbol_df['BB_Position'] = (symbol_df['Close'] - symbol_df['BB_Lower']) / (symbol_df['BB_Upper'] - symbol_df['BB_Lower'] + 1e-10)\n",
        "\n",
        "        # ATR (Average True Range)\n",
        "        high_low = symbol_df['High'] - symbol_df['Low']\n",
        "        high_close = (symbol_df['High'] - symbol_df['Close'].shift()).abs()\n",
        "        low_close = (symbol_df['Low'] - symbol_df['Close'].shift()).abs()\n",
        "        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "        symbol_df['ATR'] = true_range.rolling(window=14).mean()\n",
        "\n",
        "        # Stochastic Oscillator\n",
        "        low14 = symbol_df['Low'].rolling(window=14).min()\n",
        "        high14 = symbol_df['High'].rolling(window=14).max()\n",
        "        denom = (high14 - low14).replace(0, 1e-10)\n",
        "        symbol_df['Stoch_K'] = 100 * ((symbol_df['Close'] - low14) / denom)\n",
        "        symbol_df['Stoch_D'] = symbol_df['Stoch_K'].rolling(window=3).mean()\n",
        "\n",
        "        # Momentum\n",
        "        symbol_df['Momentum'] = symbol_df['Close'] / symbol_df['Close'].shift(10) - 1\n",
        "        symbol_df['ROC'] = symbol_df['Close'].pct_change(10) * 100\n",
        "\n",
        "        # Volume-based\n",
        "        symbol_df['Volume_SMA'] = symbol_df['Volume'].rolling(window=20).mean()\n",
        "        symbol_df['Volume_Ratio'] = symbol_df['Volume'] / (symbol_df['Volume_SMA'] + 1e-10)\n",
        "\n",
        "        # Price position in recent range\n",
        "        low_20 = symbol_df['Low'].rolling(window=20).min()\n",
        "        high_20 = symbol_df['High'].rolling(window=20).max()\n",
        "        denom_pos = (high_20 - low_20).replace(0, 1e-10)\n",
        "        symbol_df['Price_Position'] = (symbol_df['Close'] - low_20) / denom_pos\n",
        "\n",
        "        # Volatility\n",
        "        symbol_df['Volatility'] = symbol_df['Close'].pct_change().rolling(20).std()\n",
        "\n",
        "        symbol_df = symbol_df.dropna()\n",
        "        result_dfs.append(symbol_df)\n",
        "\n",
        "    final_df = pd.concat(result_dfs, ignore_index=True)\n",
        "    print(f\"final dataset: {len(final_df)} records\")\n",
        "    return final_df\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:29.995702Z",
          "iopub.execute_input": "2025-07-01T17:35:29.996019Z",
          "iopub.status.idle": "2025-07-01T17:35:30.015979Z",
          "shell.execute_reply.started": "2025-07-01T17:35:29.995993Z",
          "shell.execute_reply": "2025-07-01T17:35:30.015403Z"
        },
        "id": "Qq9kHLuXFb2B"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment and agents\n"
      ],
      "metadata": {
        "id": "INeuVU6tHn2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedTradingEnv(gym.Env):\n",
        "    \"\"\"Enhanced Gym environment for multi-stock trading\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, initial_balance=10000, max_steps=200, normalize=True):\n",
        "        super(AdvancedTradingEnv, self).__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.initial_balance = initial_balance\n",
        "        self.max_steps = min(max_steps, len(df) - 1)\n",
        "        self.normalize = normalize\n",
        "        self.current_symbol = None\n",
        "\n",
        "        # Shuffle the data\n",
        "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Feature columns\n",
        "        self.feature_columns = [\n",
        "            'Close', 'Volume', 'SMA_10', 'SMA_20', 'EMA_12', 'EMA_26',\n",
        "            'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'BB_Upper', 'BB_Lower',\n",
        "            'BB_Width', 'BB_Position', 'ATR', 'Stoch_K', 'Stoch_D',\n",
        "            'Momentum', 'ROC', 'Volume_Ratio', 'Price_Position', 'Volatility'\n",
        "        ]\n",
        "\n",
        "        self.original_data = df.copy()\n",
        "        self.scaler = None\n",
        "        self.price_scaler = None\n",
        "\n",
        "        # Normalize features\n",
        "        if self.normalize:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.price_scaler = StandardScaler()\n",
        "            available_features = [col for col in self.feature_columns if col in df.columns]\n",
        "\n",
        "            if available_features :\n",
        "                self.df[available_features] = self.scaler.fit_transform(self.df[available_features])\n",
        "            else:\n",
        "                print(\"No available features for normalization.\")\n",
        "\n",
        "        # Actions: 0 = hold, 1 = buy, 2 = sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: features + portfolio state\n",
        "        n_features = len(self.feature_columns) + 4  # +4 for shares_held, balance, net_worth , current_price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(n_features,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = float(self.initial_balance)\n",
        "        self.net_worth = float(self.initial_balance)\n",
        "        self.shares_held = 0\n",
        "        self.current_step = 0\n",
        "        self.previous_net_worth = self.initial_balance\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        if self.current_step >= len(self.df):\n",
        "            self.current_step = len(self.df) - 1\n",
        "\n",
        "        row = self.df.iloc[self.current_step]\n",
        "\n",
        "        current_price = self._get_denormalized_price(row['Close'])\n",
        "\n",
        "        # Get feature values\n",
        "        features = [row[col] for col in self.feature_columns]\n",
        "\n",
        "        max_possible_shares = self.initial_balance / max(current_price, 1)\n",
        "\n",
        "        # Add portfolio state (normalized)\n",
        "        portfolio_features = [\n",
        "            self.shares_held / max(max_possible_shares, 1),  # Normalize shares\n",
        "            (self.balance - self.initial_balance) / self.initial_balance,  # Normalize balance\n",
        "            (self.net_worth - self.initial_balance) / self.initial_balance,  # Normalize net worth\n",
        "            min(current_price / self.initial_balance, 1.0)  # Normalize current price\n",
        "        ]\n",
        "\n",
        "        obs = np.array(features + portfolio_features, dtype=np.float32)\n",
        "\n",
        "        obs = np.nan_to_num(obs, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def _get_denormalized_price(self, normalized_price):\n",
        "\n",
        "        if self.normalize and self.price_scaler is not None:\n",
        "            try:\n",
        "\n",
        "                denormalized = self.price_scaler.inverse_transform([[normalized_price]])[0][0]\n",
        "                return max(denormalized, 0.01)\n",
        "            except:\n",
        "                return max(normalized_price, 0.01)\n",
        "        return max(normalized_price, 0.01)\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= len(self.df) - 1:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        row = self.df.iloc[self.current_step]\n",
        "        current_price = self._get_denormalized_price(row['Close'])\n",
        "\n",
        "        prev_net_worth = self.net_worth\n",
        "        transaction_cost = 1e-3\n",
        "\n",
        "        reward = 0\n",
        "\n",
        "        # Execute action\n",
        "        if action == 1:  # Buy\n",
        "            if self.balance >= current_price * (1 + transaction_cost):\n",
        "                shares_to_buy = int(self.balance / (current_price * (1 + transaction_cost)))\n",
        "                if shares_to_buy > 0:\n",
        "                    cost = shares_to_buy * current_price * (1 + transaction_cost)\n",
        "                    self.shares_held += shares_to_buy\n",
        "                    self.balance -= cost\n",
        "                    reward = 0.01  # Small positive reward for successful buy\n",
        "                else:\n",
        "                    reward = -0.05\n",
        "            else:\n",
        "                reward = -0.05\n",
        "\n",
        "        elif action == 2:  # Sell\n",
        "            if self.shares_held > 0:\n",
        "                revenue = self.shares_held * current_price * (1 - transaction_cost)\n",
        "                self.balance += revenue\n",
        "                self.shares_held = 0\n",
        "                reward = 0.01\n",
        "            else:\n",
        "                reward = -0.05  # Penalty for invalid sell\n",
        "\n",
        "        # Move to next step\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps or self.current_step >= len(self.df) - 1\n",
        "\n",
        "        if not done and self.current_step < len(self.df):\n",
        "            next_row = self.df.iloc[self.current_step]\n",
        "            next_price = self._get_denormalized_price(next_row['Close'])\n",
        "            self.net_worth = self.balance + self.shares_held * next_price\n",
        "        else:\n",
        "            self.net_worth = self.balance + self.shares_held * current_price\n",
        "\n",
        "        # Calculate reward\n",
        "        portfolio_change = (self.net_worth - prev_net_worth) / self.initial_balance\n",
        "        reward += portfolio_change * 10  # Scale portfolio change\n",
        "\n",
        "        reward = np.clip(reward, -1.0, 1.0)\n",
        "\n",
        "        info = {\n",
        "            'net_worth': self.net_worth,\n",
        "            'balance': self.balance,\n",
        "            'shares': self.shares_held,\n",
        "            'current_price': current_price,\n",
        "            'action': action\n",
        "        }\n",
        "\n",
        "        return self._get_obs(), reward, done, info\n",
        "\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:30.112671Z",
          "iopub.execute_input": "2025-07-01T17:35:30.112952Z",
          "iopub.status.idle": "2025-07-01T17:35:30.126963Z",
          "shell.execute_reply.started": "2025-07-01T17:35:30.112933Z",
          "shell.execute_reply": "2025-07-01T17:35:30.126398Z"
        },
        "id": "z5LMufEmFb2C"
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedDQN(nn.Module):\n",
        "    \"\"\"Enhanced DQN with Dueling architecture\"\"\"\n",
        "    def __init__(self, input_dim, action_dim, hidden_dim=512):\n",
        "        super(ImprovedDQN, self).__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "        self.value_head = nn.Linear(hidden_dim // 2, 1)\n",
        "        self.advantage_head = nn.Linear(hidden_dim // 2, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        value = self.value_head(features)\n",
        "        advantage = self.advantage_head(features)\n",
        "\n",
        "        # Bellman\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:30.127868Z",
          "iopub.execute_input": "2025-07-01T17:35:30.128137Z",
          "iopub.status.idle": "2025-07-01T17:35:30.154783Z",
          "shell.execute_reply.started": "2025-07-01T17:35:30.128121Z",
          "shell.execute_reply": "2025-07-01T17:35:30.153963Z"
        },
        "id": "rrtUm4NkFb2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e267f389-2f3e-4cca-901c-6da42c854faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "architecture definition successful\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity=50000, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.pos = 0\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
        "\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "        else:\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
        "\n",
        "        self.priorities[self.pos] = max_prio\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            prios = self.priorities\n",
        "        else:\n",
        "            prios = self.priorities[:self.pos]\n",
        "\n",
        "        probs = prios ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return samples, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, prio in zip(indices, priorities):\n",
        "            self.priorities[idx] = prio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:35:30.155978Z",
          "iopub.execute_input": "2025-07-01T17:35:30.156282Z",
          "iopub.status.idle": "2025-07-01T17:35:30.183662Z",
          "shell.execute_reply.started": "2025-07-01T17:35:30.156263Z",
          "shell.execute_reply": "2025-07-01T17:35:30.182923Z"
        },
        "id": "dDwSnATdFb2D"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "eEm_cKBTHs2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching the data"
      ],
      "metadata": {
        "id": "f72TVNxNIXxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# symbols = ['META']\n",
        "symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'NVDA', 'META' , 'NFLX']\n",
        "print(f\"Training symbols: {symbols}\")\n",
        "\n",
        "df = fetch_data(symbols, '2018-01-01', '2023-01-01')\n",
        "\n",
        "if df.empty:\n",
        "    print(\"No data fetched.\")\n",
        "else:\n",
        "    print(f\"symbols: {len(df['Symbol'].unique())} \")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T17:39:35.897888Z",
          "iopub.execute_input": "2025-07-01T17:39:35.898294Z",
          "iopub.status.idle": "2025-07-01T17:39:36.662966Z",
          "shell.execute_reply.started": "2025-07-01T17:39:35.898268Z",
          "shell.execute_reply": "2025-07-01T17:39:36.661997Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mo9HYb9Fb2E",
        "outputId": "aba6f012-c93c-41f6-8f26-ace6fbd20971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training symbols: ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'NVDA', 'META', 'NFLX']\n",
            "âœ“ Fetched 1259 records for AAPL\n",
            "âœ“ Fetched 1259 records for GOOGL\n",
            "âœ“ Fetched 1259 records for MSFT\n",
            "âœ“ Fetched 1259 records for TSLA\n",
            "âœ“ Fetched 1259 records for AMZN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Fetched 1259 records for NVDA\n",
            "âœ“ Fetched 1259 records for META\n",
            "âœ“ Fetched 1259 records for NFLX\n",
            "\n",
            "ðŸ“Š Total combined records: 10072\n",
            "âœ… Successfully fetched data for 8 symbols\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9zfd6XbIoNG",
        "outputId": "dff02158-cb31-40b5-d5cd-cbfa1f3df8df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10072 entries, 0 to 10071\n",
            "Data columns (total 41 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   (Open, AAPL)     1259 non-null   float64\n",
            " 1   (High, AAPL)     1259 non-null   float64\n",
            " 2   (Low, AAPL)      1259 non-null   float64\n",
            " 3   (Close, AAPL)    1259 non-null   float64\n",
            " 4   (Volume, AAPL)   1259 non-null   float64\n",
            " 5   (Symbol, )       10072 non-null  object \n",
            " 6   (Open, GOOGL)    1259 non-null   float64\n",
            " 7   (High, GOOGL)    1259 non-null   float64\n",
            " 8   (Low, GOOGL)     1259 non-null   float64\n",
            " 9   (Close, GOOGL)   1259 non-null   float64\n",
            " 10  (Volume, GOOGL)  1259 non-null   float64\n",
            " 11  (Open, MSFT)     1259 non-null   float64\n",
            " 12  (High, MSFT)     1259 non-null   float64\n",
            " 13  (Low, MSFT)      1259 non-null   float64\n",
            " 14  (Close, MSFT)    1259 non-null   float64\n",
            " 15  (Volume, MSFT)   1259 non-null   float64\n",
            " 16  (Open, TSLA)     1259 non-null   float64\n",
            " 17  (High, TSLA)     1259 non-null   float64\n",
            " 18  (Low, TSLA)      1259 non-null   float64\n",
            " 19  (Close, TSLA)    1259 non-null   float64\n",
            " 20  (Volume, TSLA)   1259 non-null   float64\n",
            " 21  (Open, AMZN)     1259 non-null   float64\n",
            " 22  (High, AMZN)     1259 non-null   float64\n",
            " 23  (Low, AMZN)      1259 non-null   float64\n",
            " 24  (Close, AMZN)    1259 non-null   float64\n",
            " 25  (Volume, AMZN)   1259 non-null   float64\n",
            " 26  (Open, NVDA)     1259 non-null   float64\n",
            " 27  (High, NVDA)     1259 non-null   float64\n",
            " 28  (Low, NVDA)      1259 non-null   float64\n",
            " 29  (Close, NVDA)    1259 non-null   float64\n",
            " 30  (Volume, NVDA)   1259 non-null   float64\n",
            " 31  (Open, META)     1259 non-null   float64\n",
            " 32  (High, META)     1259 non-null   float64\n",
            " 33  (Low, META)      1259 non-null   float64\n",
            " 34  (Close, META)    1259 non-null   float64\n",
            " 35  (Volume, META)   1259 non-null   float64\n",
            " 36  (Open, NFLX)     1259 non-null   float64\n",
            " 37  (High, NFLX)     1259 non-null   float64\n",
            " 38  (Low, NFLX)      1259 non-null   float64\n",
            " 39  (Close, NFLX)    1259 non-null   float64\n",
            " 40  (Volume, NFLX)   1259 non-null   float64\n",
            "dtypes: float64(40), object(1)\n",
            "memory usage: 3.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the indicators"
      ],
      "metadata": {
        "id": "Z3-sAgEDIawF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def melt_stock_data(df):\n",
        "    df.columns = pd.MultiIndex.from_tuples([eval(col) if isinstance(col, str) else col for col in df.columns])\n",
        "\n",
        "    symbols = sorted(set(sym for (_, sym) in df.columns if sym not in (None, '')))\n",
        "    long_df = []\n",
        "\n",
        "    for sym in symbols:\n",
        "        sym_df = df.xs(key=sym, axis=1, level=1).copy()\n",
        "        sym_df['Symbol'] = sym\n",
        "        sym_df['Date'] = df.index\n",
        "        long_df.append(sym_df)\n",
        "\n",
        "    final_df = pd.concat(long_df, ignore_index=True)\n",
        "    final_df = final_df[['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "RYyjwPuqLZjn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_melted = melt_stock_data(df)\n",
        "df_with_indicators = add_indicators(df_melted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI_JhdrqHiVG",
        "outputId": "f42663ef-3103-4b03-8f80-5849590c354e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding technical indicators...\n",
            "Processing AAPL...\n",
            "Processing AMZN...\n",
            "Processing GOOGL...\n",
            "Processing META...\n",
            "Processing MSFT...\n",
            "Processing NFLX...\n",
            "Processing NVDA...\n",
            "Processing TSLA...\n",
            "âœ“ Technical indicators added. Final dataset: 9912 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7124b1b8",
        "outputId": "e75d9560-6642-4bc8-b708-ac137445a88b"
      },
      "source": [
        "print(f\"ðŸ“ˆ Dataset ready with {len(df_with_indicators.columns)} features\")\n",
        "print(f\"Feature columns: {df_with_indicators.columns.tolist()}\")\n",
        "print(\"Sample of processed data:\")\n",
        "print(df_with_indicators.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ˆ Dataset ready with 28 features\n",
            "Feature columns: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'SMA_20', 'EMA_12', 'EMA_26', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position', 'ATR', 'Stoch_K', 'Stoch_D', 'Momentum', 'ROC', 'Volume_SMA', 'Volume_Ratio', 'Price_Position', 'Volatility']\n",
            "\n",
            "ðŸ“Š Sample of processed data:\n",
            "   Date Symbol       Open       High        Low      Close       Volume  \\\n",
            "0    20   AAPL  39.161878  39.530335  39.075045  39.293301  129915600.0   \n",
            "1    21   AAPL  39.232277  39.572569  39.136055  39.375435  188923200.0   \n",
            "2    22   AAPL  38.957694  39.145443  37.573055  37.666927  346375200.0   \n",
            "3    23   AAPL  37.338362  38.460155  36.610837  36.725834  290954000.0   \n",
            "4    24   AAPL  36.336269  38.422618  36.141480  38.260685  272975200.0   \n",
            "\n",
            "      SMA_10     SMA_20     EMA_12  ...  BB_Position       ATR    Stoch_K  \\\n",
            "0  40.623021  40.856885  40.334648  ...     0.046834  0.658962  17.727309   \n",
            "1  40.353603  40.804668  40.187077  ...     0.111678  0.670026  19.999882   \n",
            "2  39.932108  40.657637  39.799361  ...    -0.145889  0.763899   1.999966   \n",
            "3  39.450768  40.440437  39.326511  ...    -0.141585  0.841513   2.033211   \n",
            "4  39.121975  40.307605  39.162538  ...     0.163640  0.934381  34.597680   \n",
            "\n",
            "     Stoch_D  Momentum        ROC   Volume_SMA  Volume_Ratio  Price_Position  \\\n",
            "0  13.099325 -0.065159  -6.515913  126824700.0      1.024371        0.177273   \n",
            "1  17.489155 -0.064041  -6.404115  130367280.0      1.449161        0.199999   \n",
            "2  13.242386 -0.100639 -10.063892  143199120.0      2.418836        0.020000   \n",
            "3   8.011020 -0.115876 -11.587600  153014820.0      1.901476        0.020332   \n",
            "4  12.876952 -0.079135  -7.913460  162550020.0      1.679330        0.345977   \n",
            "\n",
            "   Volatility  \n",
            "0    0.009462  \n",
            "1    0.009490  \n",
            "2    0.013250  \n",
            "3    0.013567  \n",
            "4    0.017207  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "B8jGycilIi_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_multi_stock_dqn(df, episodes=1000, max_steps=300):\n",
        "\n",
        "    gamma = 0.99\n",
        "    epsilon_start = 1.0\n",
        "    epsilon_end = 0.05\n",
        "    epsilon_decay = 0.9995\n",
        "    batch_size = 32\n",
        "    target_update = 20\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    print(f\"Training Configuration:\")\n",
        "    print(f\"   Episodes: {episodes}\")\n",
        "    print(f\"   Max steps per episode: {max_steps}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Learning rate: {learning_rate}\")\n",
        "\n",
        "    # Initialize environment\n",
        "    env = AdvancedTradingEnv(df, initial_balance=10000, max_steps=max_steps, normalize=True)\n",
        "    print(f\"   Observation space: {env.observation_space.shape}\")\n",
        "    print(f\"   Action space: {env.action_space.n}\")\n",
        "\n",
        "    # Initialize networks\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    policy_net = ImprovedDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    target_net = ImprovedDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)\n",
        "    memory = PrioritizedReplayBuffer(capacity=50000)\n",
        "\n",
        "    # Training loop\n",
        "    epsilon = epsilon_start\n",
        "    episode_rewards = []\n",
        "    episode_net_worths = []\n",
        "    action_counts = [0, 0, 0]\n",
        "    best_reward = float('-inf')\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        episode_actions = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randrange(env.action_space.n)\n",
        "            else:\n",
        "                policy_net.eval() # Set policy_net to evaluation mode\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                    if len(state_tensor.shape) == 1:\n",
        "                        state_tensor = state_tensor.unsqueeze(0)\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = q_values.argmax().item()\n",
        "                policy_net.train() # Set policy_net back to training mode\n",
        "\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            if np.any(np.isnan(state)) or np.any(np.isnan(next_state)):\n",
        "                continue\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            episode_actions.append(action)\n",
        "            action_counts[action] += 1\n",
        "\n",
        "            # Training\n",
        "            if len(memory) >= batch_size*2:\n",
        "                samples, indices, weights = memory.sample(batch_size)\n",
        "\n",
        "                states = torch.FloatTensor([s[0] for s in samples]).to(device)\n",
        "                actions = torch.LongTensor([s[1] for s in samples]).to(device)\n",
        "                rewards = torch.FloatTensor([s[2] for s in samples]).to(device)\n",
        "                next_states = torch.FloatTensor([s[3] for s in samples]).to(device)\n",
        "                dones = torch.BoolTensor([s[4] for s in samples]).to(device)\n",
        "                weights_tensor = torch.FloatTensor(weights).to(device)\n",
        "\n",
        "                current_q_values = policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "                next_q_values = target_net(next_states).max(1)[0].detach()\n",
        "                target_q_values = rewards + (gamma * next_q_values * ~dones)\n",
        "\n",
        "                # Compute loss with importance sampling weights\n",
        "                td_errors = current_q_values.squeeze() - target_q_values\n",
        "                loss = (weights_tensor * td_errors.pow(2)).mean()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update priorities\n",
        "                priorities = np.abs(td_errors.detach().cpu().numpy()) + 1e-6\n",
        "                memory.update_priorities(indices, priorities)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update target network\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        # Decay epsilon and update scheduler\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track performance\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_net_worths.append(info.get('net_worth', env.initial_balance))\n",
        "\n",
        "        # Save best model\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(policy_net.state_dict(), 'best_dqn_model.pth')\n",
        "\n",
        "        # Progress reporting\n",
        "        if episode % 25 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            avg_net_worth = np.mean(episode_net_worths[-50:])\n",
        "            action_dist = [c/sum(action_counts)*100 for c in action_counts]\n",
        "            print(f\"Episode {episode:4d}/{episodes} | \"\n",
        "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
        "                  f\"Avg Net Worth: ${avg_net_worth:8.2f} | \"\n",
        "                  f\"Epsilon: {epsilon:.3f} | \"\n",
        "                  f\"Actions: H{action_dist[0]:.1f}% B{action_dist[1]:.1f}% S{action_dist[2]:.1f}%\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Final action distribution: Hold {action_counts[0]}, Buy {action_counts[1]}, Sell {action_counts[2]}\")\n",
        "    return policy_net, env, episode_rewards, episode_net_worths"
      ],
      "metadata": {
        "id": "yhrMdQULL4lA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_environment(df, initial_balance=10000, max_steps=10):\n",
        "    \"\"\"Debug the environment to understand action behavior\"\"\"\n",
        "    env = AdvancedTradingEnv(df, initial_balance=initial_balance, max_steps=max_steps, normalize=True)\n",
        "\n",
        "    print(\"=== ENVIRONMENT DEBUG ===\")\n",
        "    print(f\"Initial balance: ${initial_balance}\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Max steps: {max_steps}\")\n",
        "\n",
        "    state = env.reset()\n",
        "    print(f\"Initial state shape: {state.shape}\")\n",
        "    print(f\"Initial state: {state}\")\n",
        "    print(f\"Initial net worth: ${env.net_worth}\")\n",
        "    print(f\"Initial balance: ${env.balance}\")\n",
        "    print(f\"Initial shares: {env.shares_held}\")\n",
        "\n",
        "    for step in range(min(5, max_steps)):\n",
        "        for action in range(3):\n",
        "            # Save state\n",
        "            saved_balance = env.balance\n",
        "            saved_shares = env.shares_held\n",
        "            saved_step = env.current_step\n",
        "\n",
        "            # Try action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            print(f\"\\nStep {step}, Action {action} ({'Hold' if action==0 else 'Buy' if action==1 else 'Sell'}):\")\n",
        "            print(f\"  Reward: {reward:.4f}\")\n",
        "            print(f\"  Net worth: ${info['net_worth']:.2f}\")\n",
        "            print(f\"  Balance: ${info['balance']:.2f}\")\n",
        "            print(f\"  Shares: {info['shares']}\")\n",
        "            print(f\"  Price: ${info['current_price']:.2f}\")\n",
        "            print(f\"  Done: {done}\")\n",
        "\n",
        "            # Reset state for next action test\n",
        "            env.balance = saved_balance\n",
        "            env.shares_held = saved_shares\n",
        "            env.current_step = saved_step\n",
        "\n",
        "        # Actually take hold action to advance\n",
        "        env.step(0)\n",
        "        if done:\n",
        "            break"
      ],
      "metadata": {
        "id": "hUu1uvuLEjQX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_environment(df_with_indicators, initial_balance=10000, max_steps=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyHbU4T8FCQt",
        "outputId": "054b82e8-2c07-4daa-d847-8c86f1df6425"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENVIRONMENT DEBUG ===\n",
            "Initial balance: $10000\n",
            "Data shape: (9912, 28)\n",
            "Max steps: 10\n",
            "Initial state shape: (26,)\n",
            "Initial state: [ 3.5891861e-01 -2.5954154e-01  3.5925537e-01  3.7596849e-01\n",
            "  3.8079530e-01  4.2420346e-01  4.8856180e-02 -9.0987808e-01\n",
            " -1.3026569e+00  1.1251770e+00  4.1165644e-01  3.3114782e-01\n",
            "  4.0938827e-01 -2.9675043e-01  6.5532506e-01  2.9986805e-01\n",
            "  8.3912653e-01  4.5360580e-01  4.5360580e-01 -5.5934042e-01\n",
            " -5.4327786e-01  2.7346188e-01  0.0000000e+00  0.0000000e+00\n",
            "  0.0000000e+00  3.5891862e-05]\n",
            "Initial net worth: $10000.0\n",
            "Initial balance: $10000.0\n",
            "Initial shares: 0\n",
            "\n",
            "Step 0, Action 0 (Hold):\n",
            "  Reward: 0.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.36\n",
            "  Done: False\n",
            "\n",
            "Step 0, Action 1 (Buy):\n",
            "  Reward: 1.0000\n",
            "  Net worth: $28324.64\n",
            "  Balance: $0.23\n",
            "  Shares: 27833\n",
            "  Price: $0.36\n",
            "  Done: False\n",
            "\n",
            "Step 0, Action 2 (Sell):\n",
            "  Reward: -1.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.36\n",
            "  Done: False\n",
            "\n",
            "Step 1, Action 0 (Hold):\n",
            "  Reward: 0.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $1.02\n",
            "  Done: False\n",
            "\n",
            "Step 1, Action 1 (Buy):\n",
            "  Reward: -1.0000\n",
            "  Net worth: $98.86\n",
            "  Balance: $0.70\n",
            "  Shares: 9816\n",
            "  Price: $1.02\n",
            "  Done: False\n",
            "\n",
            "Step 1, Action 2 (Sell):\n",
            "  Reward: 1.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $1.02\n",
            "  Done: False\n",
            "\n",
            "Step 2, Action 0 (Hold):\n",
            "  Reward: 0.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.01\n",
            "  Done: False\n",
            "\n",
            "Step 2, Action 1 (Buy):\n",
            "  Reward: 0.0000\n",
            "  Net worth: $9990.01\n",
            "  Balance: $0.01\n",
            "  Shares: 999000\n",
            "  Price: $0.01\n",
            "  Done: False\n",
            "\n",
            "Step 2, Action 2 (Sell):\n",
            "  Reward: -0.0400\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.01\n",
            "  Done: False\n",
            "\n",
            "Step 3, Action 0 (Hold):\n",
            "  Reward: 0.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.01\n",
            "  Done: False\n",
            "\n",
            "Step 3, Action 1 (Buy):\n",
            "  Reward: 1.0000\n",
            "  Net worth: $827630.82\n",
            "  Balance: $0.01\n",
            "  Shares: 999000\n",
            "  Price: $0.01\n",
            "  Done: False\n",
            "\n",
            "Step 3, Action 2 (Sell):\n",
            "  Reward: -1.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.01\n",
            "  Done: False\n",
            "\n",
            "Step 4, Action 0 (Hold):\n",
            "  Reward: 0.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.83\n",
            "  Done: False\n",
            "\n",
            "Step 4, Action 1 (Buy):\n",
            "  Reward: -1.0000\n",
            "  Net worth: $6355.90\n",
            "  Balance: $0.45\n",
            "  Shares: 12058\n",
            "  Price: $0.83\n",
            "  Done: False\n",
            "\n",
            "Step 4, Action 2 (Sell):\n",
            "  Reward: 1.0000\n",
            "  Net worth: $10000.00\n",
            "  Balance: $10000.00\n",
            "  Shares: 0\n",
            "  Price: $0.83\n",
            "  Done: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_env, training_rewards, training_net_worths = train_multi_stock_dqn(\n",
        "    df_with_indicators,\n",
        "    episodes=500,\n",
        "    max_steps=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIUDLkGyMGku",
        "outputId": "79c67b9c-4fcb-406a-822a-39e4fcce794a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Configuration:\n",
            "   Episodes: 500\n",
            "   Max steps per episode: 50\n",
            "   Batch size: 32\n",
            "   Learning rate: 0.0001\n",
            "   Observation space: (26,)\n",
            "   Action space: 3\n",
            "   Device: cuda\n",
            "Starting training...\n",
            "Episode    0/500 | Avg Reward:    -1.48 | Avg Net Worth: $ 1139.08 | Epsilon: 1.000 | Actions: H34.0% B30.0% S36.0%\n",
            "Episode   25/500 | Avg Reward:    -3.68 | Avg Net Worth: $812665626.43 | Epsilon: 0.987 | Actions: H30.3% B34.0% S35.7%\n",
            "Episode   50/500 | Avg Reward:    -3.83 | Avg Net Worth: $1113166913.96 | Epsilon: 0.975 | Actions: H32.4% B34.3% S33.4%\n",
            "Episode   75/500 | Avg Reward:    -3.32 | Avg Net Worth: $791495108.25 | Epsilon: 0.963 | Actions: H32.4% B34.9% S32.7%\n",
            "Episode  100/500 | Avg Reward:    -3.29 | Avg Net Worth: $29621047935.27 | Epsilon: 0.951 | Actions: H32.5% B34.9% S32.6%\n",
            "Episode  125/500 | Avg Reward:    -3.64 | Avg Net Worth: $29972530949.12 | Epsilon: 0.939 | Actions: H32.1% B35.7% S32.1%\n",
            "Episode  150/500 | Avg Reward:    -3.47 | Avg Net Worth: $1209997915.24 | Epsilon: 0.927 | Actions: H31.9% B36.0% S32.1%\n",
            "Episode  175/500 | Avg Reward:    -3.55 | Avg Net Worth: $780132953.51 | Epsilon: 0.916 | Actions: H32.0% B35.9% S32.1%\n",
            "Episode  200/500 | Avg Reward:    -3.56 | Avg Net Worth: $276914521.12 | Epsilon: 0.904 | Actions: H32.0% B35.6% S32.4%\n",
            "Episode  225/500 | Avg Reward:    -3.94 | Avg Net Worth: $575806449.04 | Epsilon: 0.893 | Actions: H32.0% B35.0% S33.0%\n",
            "Episode  250/500 | Avg Reward:    -3.59 | Avg Net Worth: $71880497096.27 | Epsilon: 0.882 | Actions: H32.8% B34.5% S32.8%\n",
            "Episode  275/500 | Avg Reward:    -3.29 | Avg Net Worth: $72061558771.39 | Epsilon: 0.871 | Actions: H33.4% B34.0% S32.6%\n",
            "Episode  300/500 | Avg Reward:    -3.07 | Avg Net Worth: $514665507.71 | Epsilon: 0.860 | Actions: H34.0% B33.9% S32.2%\n",
            "Episode  325/500 | Avg Reward:    -2.52 | Avg Net Worth: $21373868.18 | Epsilon: 0.850 | Actions: H34.5% B33.5% S32.0%\n",
            "Episode  350/500 | Avg Reward:    -2.70 | Avg Net Worth: $19319841.89 | Epsilon: 0.839 | Actions: H35.1% B33.1% S31.8%\n",
            "Episode  375/500 | Avg Reward:    -3.16 | Avg Net Worth: $13054281.71 | Epsilon: 0.829 | Actions: H35.5% B32.9% S31.7%\n",
            "Episode  400/500 | Avg Reward:    -3.15 | Avg Net Worth: $163440504.92 | Epsilon: 0.818 | Actions: H36.0% B32.6% S31.4%\n",
            "Episode  425/500 | Avg Reward:    -2.71 | Avg Net Worth: $164102822.98 | Epsilon: 0.808 | Actions: H36.6% B32.4% S31.0%\n",
            "Episode  450/500 | Avg Reward:    -2.72 | Avg Net Worth: $29082571.19 | Epsilon: 0.798 | Actions: H37.0% B32.2% S30.8%\n",
            "Episode  475/500 | Avg Reward:    -2.65 | Avg Net Worth: $205802934.68 | Epsilon: 0.788 | Actions: H37.5% B31.9% S30.6%\n",
            "Training completed!\n",
            "Final action distribution: Hold 9447, Buy 7921, Sell 7632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "J9iYtr7GNxIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(policy_net, test_symbols, start_date, end_date , initial_balance = 10000 , max_steps = 200):\n",
        "    # Fetch test data\n",
        "    test_df = fetch_data(test_symbols, start_date, end_date)\n",
        "    test_df = melt_stock_data(test_df)\n",
        "    test_df = add_indicators(test_df)\n",
        "\n",
        "    if initial_balance <= 0:\n",
        "        initial_balance = 10000\n",
        "        print(\"initial_balance value reset to 10,000\")\n",
        "\n",
        "    # Create test environment\n",
        "    test_env = AdvancedTradingEnv(test_df, initial_balance=10000, max_steps=200, normalize=True)\n",
        "\n",
        "    # Test the trained model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    policy_net.to(device)\n",
        "    state = test_env.reset()\n",
        "    dqn_values = [test_env.initial_balance]\n",
        "    actions_taken = []\n",
        "    prices = []\n",
        "\n",
        "    policy_net.eval()\n",
        "    with torch.no_grad():\n",
        "        for step in range(min(max_steps, len(test_df) - 1)):\n",
        "            try:\n",
        "\n",
        "                state_tensor = torch.FloatTensor(state).to(device)\n",
        "                if len(state_tensor.shape) == 1:\n",
        "                    state_tensor = state_tensor.unsqueeze(0)\n",
        "                q_values = policy_net(state_tensor)\n",
        "\n",
        "                # Use epsilon-greedy algorithm\n",
        "                if random.random() < 0.1:  # 10% random actions\n",
        "                    action = random.randrange(test_env.action_space.n)\n",
        "                else:\n",
        "                    action = q_values.argmax().item()\n",
        "\n",
        "                next_state, reward, done, info = test_env.step(action)\n",
        "\n",
        "                if np.any(np.isnan(next_state)):\n",
        "                    print(f\"Warning: NaN detected in state at step {step}\")\n",
        "                    break\n",
        "\n",
        "                state = next_state\n",
        "                dqn_values.append(info['net_worth'])\n",
        "                actions_taken.append(action)\n",
        "                prices.append(info['current_price'])\n",
        "\n",
        "                if step % 50 == 0:\n",
        "                    print(f\"Step {step}: Action={action}, Price=${info['current_price']:.2f}, \"\n",
        "                          f\"Net Worth=${info['net_worth']:.2f}, Balance=${info['balance']:.2f}, \"\n",
        "                          f\"Shares={info['shares']}\")\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error at step {step}: {e}\")\n",
        "                break\n",
        "\n",
        "    # Calculate buy-and-hold baseline\n",
        "    if prices:\n",
        "        initial_price = prices[0]\n",
        "        shares_bh = initial_balance / initial_price\n",
        "        buy_hold_values = [shares_bh * price for price in prices]\n",
        "        # Align lengths\n",
        "        min_len = min(len(dqn_values), len(buy_hold_values))\n",
        "        dqn_values = dqn_values[:min_len]\n",
        "        buy_hold_values = buy_hold_values[:min_len]\n",
        "    else:\n",
        "        buy_hold_values = [initial_balance] * len(dqn_values)\n",
        "\n",
        "    print(f\"evaluation completed on {len(dqn_values)} time steps\")\n",
        "    print(f\"Action distribution: Hold {actions_taken.count(0)}, Buy {actions_taken.count(1)}, Sell {actions_taken.count(2)}\")\n",
        "\n",
        "    return dqn_values, buy_hold_values, actions_taken"
      ],
      "metadata": {
        "id": "opRft93VM2oe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "test_symbols = ['AAPL', 'GOOGL', 'MSFT']  # Subset for testing\n",
        "dqn_values, buy_hold_values, actions = evaluate_model(\n",
        "    trained_model, test_symbols, '2022-01-01', '2023-01-01' , initial_balance = 0 , max_steps = 500\n",
        ")"
      ],
      "metadata": {
        "id": "3ky_hnK4M7k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92b3039-a9bc-4edc-a372-04c695b5fa66"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Fetched 251 records for AAPL\n",
            "âœ“ Fetched 251 records for GOOGL\n",
            "âœ“ Fetched 251 records for MSFT\n",
            "\n",
            "ðŸ“Š Total combined records: 753\n",
            "Adding technical indicators...\n",
            "Processing AAPL...\n",
            "Processing GOOGL...\n",
            "Processing MSFT...\n",
            "âœ“ Technical indicators added. Final dataset: 693 records\n",
            "initial_balance value reset to 10,000\n",
            "Step 0: Action=2, Price=$0.01, Net Worth=$10000.00, Balance=$10000.00, Shares=0\n",
            "Step 50: Action=0, Price=$0.01, Net Worth=$9990.01, Balance=$0.01, Shares=999000\n",
            "Step 100: Action=0, Price=$0.01, Net Worth=$1177746.77, Balance=$0.01, Shares=999000\n",
            "Step 150: Action=0, Price=$1.18, Net Worth=$9990.01, Balance=$0.01, Shares=999000\n",
            "âœ… Evaluation completed on 200 time steps\n",
            "Action distribution: Hold 187, Buy 6, Sell 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(dqn_values, buy_hold_values, actions):\n",
        "    \"\"\"Plot results with action annotations\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Portfolio comparison\n",
        "    axes[0,0].plot(dqn_values, label='DQN Agent', linewidth=2)\n",
        "    axes[0,0].plot(buy_hold_values, label='Buy & Hold', linewidth=2)\n",
        "    axes[0,0].set_title('Portfolio Value Comparison')\n",
        "    axes[0,0].set_xlabel('Time Steps')\n",
        "    axes[0,0].set_ylabel('Portfolio Value ($)')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Action distribution\n",
        "    action_names = ['Hold', 'Buy', 'Sell']\n",
        "    action_counts = [actions.count(i) for i in range(3)]\n",
        "\n",
        "    # Handle case where no actions were taken\n",
        "    if sum(action_counts) == 0:\n",
        "        action_counts = [1, 1, 1]  # Equal distribution for visualization\n",
        "\n",
        "    axes[0,1].pie(action_counts, labels=action_names, autopct='%1.1f%%')\n",
        "    axes[0,1].set_title('Action Distribution')\n",
        "\n",
        "    # Actions over time\n",
        "    if actions:\n",
        "        axes[1,0].plot(actions, 'o-', alpha=0.6, markersize=2)\n",
        "        axes[1,0].set_title('Actions Over Time')\n",
        "        axes[1,0].set_xlabel('Time Steps')\n",
        "        axes[1,0].set_ylabel('Action (0=Hold, 1=Buy, 2=Sell)')\n",
        "        axes[1,0].set_ylim(-0.5, 2.5)\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance metrics\n",
        "    if len(dqn_values) > 1 and len(buy_hold_values) > 1:\n",
        "        dqn_return = (dqn_values[-1] - dqn_values[0]) / dqn_values[0] * 100\n",
        "        bh_return = (buy_hold_values[-1] - buy_hold_values[0]) / buy_hold_values[0] * 100\n",
        "\n",
        "        metrics = ['Total Return (%)', 'Final Value ($)']\n",
        "        dqn_metrics = [dqn_return, dqn_values[-1]]\n",
        "        bh_metrics = [bh_return, buy_hold_values[-1]]\n",
        "\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "\n",
        "        axes[1,1].bar(x - width/2, dqn_metrics, width, label='DQN Agent', alpha=0.8)\n",
        "        axes[1,1].bar(x + width/2, bh_metrics, width, label='Buy & Hold', alpha=0.8)\n",
        "        axes[1,1].set_title('Performance Metrics')\n",
        "        axes[1,1].set_xticks(x)\n",
        "        axes[1,1].set_xticklabels(metrics)\n",
        "        axes[1,1].legend()\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    if len(dqn_values) > 1 and len(buy_hold_values) > 1:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(\"PERFORMANCE SUMMARY\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"DQN Agent Return: {dqn_return:.2f}%\")\n",
        "        print(f\"Buy & Hold Return: {bh_return:.2f}%\")\n",
        "        print(f\"Outperformance: {dqn_return - bh_return:.2f}%\")\n",
        "        print(f\"Final DQN Value: ${dqn_values[-1]:.2f}\")\n",
        "        print(f\"Final B&H Value: ${buy_hold_values[-1]:.2f}\")\n",
        "        print(f\"Action Counts: Hold={action_counts[0]}, Buy={action_counts[1]}, Sell={action_counts[2]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Fc5fUg289jbb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_results(dqn_values, buy_hold_values, actions)"
      ],
      "metadata": {
        "id": "sjCA-KL8XdxO"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}